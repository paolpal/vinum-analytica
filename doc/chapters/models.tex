\section{Models}
Three different categories of models were
considered:
\begin{itemize}
    \item \textit{Decision Trees}
    \item \textit{Random Forests}
    \item \textit{Neural Networks}
    \begin{itemize}
        \item \textit{Feedforward Networks}
        \item \textit{LSTM Networks}
    \end{itemize}
\end{itemize}
Decision Trees and Random Forests both use
the BoW preprocessing with \textbf{binary} 
encoding.
Feedforward Neural Networks use the BoW
preprocessing with \textbf{TFIDF} encoding.
LSTM Networks were tested with both the
WE and GE preprocessing.

\subsection{Imbalanced Dataset}
To handle the imbalanced dataset, a class
weight was assigned to each class equal to
$W_c=\frac{|D|}{|C||D_c|}$ where $|D|$ is
the size of the dataset, $|C|$ is the number
of classes and $|D_c|$ is the number of
samples in class $c$. This weight was used in
the criterion of the Decision Trees and Random
Forests models, and in the loss function of
the Neural Networks models.

\subsection{Model Selection}
To find a good hyperparameter configuration
for each model, a random search was performed.
The search was done with 10 iterations for
each model (Decision Trees, Random Forests 
and Neural Networks). Each configuration was
evaluated with a 6-fold cross-validation on
the training set. Two subsequent random
searches were performed, the second one
narrowing the search space around the best
configuration found in the first search.
The search spaces for each model are shown
in \autoref{tab:dt_search_spaces_1}, 
\autoref{tab:rf_search_spaces_1} and 
\autoref{tab:nn_search_spaces_1} for the 
first search, and in 
\autoref{tab:dt_search_spaces_2},
\autoref{tab:rf_search_spaces_2} and
\autoref{tab:nn_search_spaces_2} for the
second search.

%\input{chapters/hypers}

Accuracy was used as the metric to evaluate
the models.